{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import argparse\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of main script ....\n",
    "skip_gram = 1\n",
    "win_size = 20\n",
    "dim = 50\n",
    "len_seq = 200\n",
    "perm = 'permutation_1_6_label'\n",
    "#Load train/val/test subject list with labels\n",
    "train_list = pd.read_csv('train_list_with_labels.csv', sep = '\\t') #set_learning/\n",
    "val_list = pd.read_csv('val_list_with_labels.csv', sep = '\\t')  #set_learning/\n",
    "test_list = pd.read_csv('test_list_with_labels.csv', sep = '\\t') #set_learning/\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Train w2v model under two scenarios #\n",
    "#######################################\n",
    "#with open(\"all_sequences_abnorm.txt\", \"r\") as text_file:  #set_learning/\n",
    "        #sequences = [i.replace('\\n','') for i in text_file]   \n",
    "#sequences = [i.split(' ') for i in sequences]\n",
    "ori_sequences = pd.read_csv(\"all_train.csv\")\n",
    "perm = 'noperm'\n",
    "if perm == 'noperm':\n",
    "    sequences = [i.split(' ') for i in list(ori_sequences.seq)]\n",
    "else: #othe perm only cases...\n",
    "    perm_sequences = pd.read_csv(perm + \".csv\")\n",
    "    sequences = [i.split(' ') for i in list(perm_sequences.seq)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Load train/val data and labels under two scenarios #\n",
    "######################################################\n",
    "\n",
    "#1) when no permutation\n",
    "if perm == 'noperm':\n",
    "\ttrain_data = pd.read_csv('train_lab_abnorm_sc1.csv', sep = ',') #set_learning/\n",
    "\t#train_data = ori_sequences[ori_sequences.subject_id.isin(train_list.subject_id)] \n",
    "\tval_data = pd.read_csv('val_lab_abnorm_sc1.csv', sep = ',') \n",
    "\n",
    "\t#val_data = pd.read_csv('val_lab_abnorm.csv', sep = ',') #set_learning/\n",
    "\ttest_data = pd.read_csv('test_lab_abnorm_sc1.csv', sep = ',')\n",
    "\n",
    "\t#train_y = list(train_list.HF)\n",
    "\t#val_y = list(val_list.HF)\n",
    "\ttrain_y = list(train_data.HF)#[int(train_list[train_list['subject_id']==i].HF) for i in train_data.subject_id]\n",
    "\tval_y = list(val_data.HF)#[int(val_list[val_list['subject_id']==i].HF) for i in val_data.subject_id]\n",
    "\ttest_y = list(test_data.HF)#[int(test_list[test_list['subject_id']==i].HF) for i in test_data.subject_id]\n",
    "    \n",
    "\n",
    "#2) when permutation\n",
    "elif perm == 'both':\n",
    "    \n",
    "\ttrain_data_1 = pd.read_csv('train_lab_abnorm.csv', sep = ',')  #set_learning/\n",
    "\tval_data_1 = pd.read_csv('val_lab_abnorm.csv', sep = ',')#set_learning/\n",
    "\t\n",
    "\ttrain_y_1 = list(train_list.HF)\n",
    "\tval_y_1 = list(val_list.HF)\n",
    "\n",
    "\ttrain_data_2 = perm_sequences[perm_sequences.subject_id.isin(train_list.subject_id)]\n",
    "\tval_data_2 = perm_sequences[perm_sequences.subject_id.isin(val_list.subject_id)]\n",
    "\tdel perm_sequences\n",
    "\n",
    "\ttrain_y_2 = [int(train_list[train_list['subject_id']==i].HF) for i in train_data_2.subject_id]\n",
    "\tval_y_2 = [int(val_list[val_list['subject_id']==i].HF) for i in val_data_2.subject_id]\n",
    "\t#combine the two parts\n",
    "\t\n",
    "\ttrain_data = pd.concat([train_data_1[['subject_id','seq']], train_data_2[['subject_id','seq']]])\n",
    "\tval_data = pd.concat([val_data_1[['subject_id','seq']], val_data_2[['subject_id','seq']]])\n",
    "\n",
    "\ttrain_y = train_y_1 + train_y_2\n",
    "\tval_y = val_y_1 + val_y_2\n",
    "\n",
    "else: #other perm cases...\n",
    "\n",
    "\tperm_sequences = pd.read_csv(perm+\".csv\")\n",
    "\ttrain_data = perm_sequences[perm_sequences.subject_id.isin(train_list.subject_id)]\n",
    "\t#train_data = pd.read_csv('train_lab_abnorm.csv', sep = ',')\n",
    "\tval_data = perm_sequences[perm_sequences.subject_id.isin(val_list.subject_id)]\n",
    "\ttest_data = ori_sequences[ori_sequences.subject_id.isin(test_list.subject_id)]\n",
    "\n",
    "\tdel perm_sequences\n",
    "\n",
    "\ttrain_y = list(train_data.HF)#[int(train_list[train_list['subject_id']==i].HF) for i in train_data.subject_id]\n",
    "\tval_y = list(val_data.HF)#[int(val_list[val_list['subject_id']==i].HF) for i in val_data.subject_id]\n",
    "\ttest_y = list(test_data.HF)#[int(test_list[test_list['subject_id']==i].HF) for i in test_data.subject_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trun_sequences(seq, len_seq):\n",
    "    new_seq = list()\n",
    "    for i in seq:\n",
    "        #print(len(i))\n",
    "        length= len(i)\n",
    "        if length>len_seq:\n",
    "             new_seq =  new_seq + [i[length-len_seq-1:-1]]\n",
    "        if length<=len_seq:\n",
    "             new_seq =  new_seq + [['00000']*(len_seq-length) +i]\n",
    "                \n",
    "    return new_seq          \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "train_x = pad_trun_sequences([i.split(' ') for i in train_data.seq][:train_data.count()[0]], len_seq )\n",
    "#del train_data\n",
    "val_x = pad_trun_sequences([i.split(' ') for i in val_data.seq][:val_data.count()[0]], len_seq )\n",
    "#del val_data \n",
    "test_x = pad_trun_sequences([i.split(' ') for i in test_data.seq][:test_data.count()[0]], len_seq )\n",
    "#del test_data\n",
    "#[i.split(' ') for i in train_data.seq][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = gensim.models.Word2Vec(train_x+val_x+test_x, sg=skip_gram, window = win_size, iter=5, size= dim, min_count=1, workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 188 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embedding_matrix = np.zeros((len(_model.wv.vocab) + 1, dim))\n",
    "\n",
    "for i, word in enumerate(_model.wv.vocab):\n",
    "    coefs = np.asarray(_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    embedding_matrix[i] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37235039, -0.17826508, -0.46533552, ...,  0.84098381,\n",
       "         0.15317829,  0.14197485],\n",
       "       [ 0.2335694 , -0.26351935,  0.06532324, ...,  0.26811111,\n",
       "        -0.05035434,  0.09372307],\n",
       "       [ 0.22731383, -0.24896312,  0.07727272, ...,  0.43054605,\n",
       "         0.02942917,  0.43146661],\n",
       "       ...,\n",
       "       [-0.1878562 ,  0.29569817, -0.10441412, ...,  0.0747788 ,\n",
       "        -0.28476393, -0.03737698],\n",
       "       [ 0.13212186, -0.07086154,  0.09452537, ...,  0.03970203,\n",
       "        -0.40025756, -0.33719414],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "for i, word in enumerate(_model.wv.vocab):\n",
    "    word_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(seq):\n",
    "    seq_encoded = list()\n",
    "    for i in seq:\n",
    "        seq_encoded = seq_encoded + [[word_index[j] for j in i]]       \n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_encoded = encoder(train_x)\n",
    "val_x_encoded = encoder(val_x)\n",
    "test_x_encoded = encoder(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5426 samples, validate on 904 samples\n",
      "Epoch 1/10\n",
      "5426/5426 [==============================] - 2s 424us/step - loss: 0.6208 - acc: 0.6587 - val_loss: 0.5954 - val_acc: 0.6748\n",
      "Epoch 2/10\n",
      "5426/5426 [==============================] - 2s 281us/step - loss: 0.5853 - acc: 0.6756 - val_loss: 0.5777 - val_acc: 0.6991\n",
      "Epoch 3/10\n",
      "5426/5426 [==============================] - 2s 283us/step - loss: 0.5750 - acc: 0.6924 - val_loss: 0.5721 - val_acc: 0.6858\n",
      "Epoch 4/10\n",
      "5426/5426 [==============================] - 2s 286us/step - loss: 0.5649 - acc: 0.6976 - val_loss: 0.5707 - val_acc: 0.6936\n",
      "Epoch 5/10\n",
      "5426/5426 [==============================] - 2s 304us/step - loss: 0.5511 - acc: 0.7145 - val_loss: 0.5742 - val_acc: 0.7069\n",
      "Epoch 6/10\n",
      "5426/5426 [==============================] - 2s 288us/step - loss: 0.5324 - acc: 0.7250 - val_loss: 0.5912 - val_acc: 0.6969\n",
      "Epoch 7/10\n",
      "5426/5426 [==============================] - 2s 283us/step - loss: 0.5214 - acc: 0.7396 - val_loss: 0.6116 - val_acc: 0.7179\n",
      "Epoch 8/10\n",
      "5426/5426 [==============================] - 2s 284us/step - loss: 0.4998 - acc: 0.7501 - val_loss: 0.6186 - val_acc: 0.6692\n",
      "Epoch 9/10\n",
      "5426/5426 [==============================] - 2s 284us/step - loss: 0.4823 - acc: 0.7680 - val_loss: 0.6310 - val_acc: 0.7013\n",
      "Epoch 10/10\n",
      "5426/5426 [==============================] - 2s 284us/step - loss: 0.4545 - acc: 0.7776 - val_loss: 0.6569 - val_acc: 0.6925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff9ba5e4b38>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input\n",
    "from keras.layers import LSTM,GRU,Conv1D,MaxPooling1D,Flatten,Embedding,LeakyReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras_metrics\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "ksize = 3\n",
    "\n",
    "\n",
    "sequence_input = Input(shape=(len_seq,), dtype='int32')\n",
    "embedded_sequences = Embedding(len(embeddings_index) + 1, dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=len_seq,\n",
    "                            trainable=False)(sequence_input)\n",
    "x = Conv1D(cnn_dim, kernel_size=ksize, strides=ksize, padding='valid', dilation_rate=1,  activation=None, use_bias=True)(embedded_sequences)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Conv1D(cnn_dim, kernel_size=ksize, strides=ksize, padding='valid',dilation_rate=1,  activation=None, use_bias=True)(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Conv1D(cnn_dim, kernel_size=ksize, strides=ksize, padding='valid', dilation_rate=1, activation=None, use_bias=True)(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)  # global max pooling\n",
    "x = GRU(cnn_dim,dropout = 0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "#model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(np.array(train_x_encoded),train_y, validation_data=(np.array(val_x_encoded), val_y),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5426, 200)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_x_encoded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7fb163839358>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#sequence_input = Input(shape=(len_seq,), dtype='ufloat32')\n",
    "#embedded_sequences = embedding_layer(sequence_input)\n",
    "model.add(Embedding(len(embeddings_index) + 1, dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=len_seq,\n",
    "                            trainable=False))\n",
    " ksize=3\n",
    "    i = Input(batch_shape=(batch_size, timesteps, input_dim))\n",
    "    o = Conv1D(cnn_dim, kernel_size=ksize, strides=ksize, padding='valid', dilation_rate=1,  activation=None, use_bias=True)(i)\n",
    "    o = LeakyReLU(alpha=0.3)(o)\n",
    "    o = Conv1D(cnn_dim, kernel_size=ksize, strides=ksize, padding='valid',dilation_rate=1,  activation=None, use_bias=True)(i)\n",
    "    o = LeakyReLU(alpha=0.3)(o)\n",
    "    o = Conv1D(cnn_dim, kernel_size=ksize, strides=ksize, padding='valid', dilation_rate=1, activation=None, use_bias=True)(i)\n",
    "    o = LeakyReLU(alpha=0.3)(o)\n",
    "    o = GRU(cnn_dim,dropout = 0.2)(o)\n",
    "    o = Dense(1,activation='sigmoid')(o)\n",
    "\n",
    "    m = Model(inputs=[i], outputs=[o])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
